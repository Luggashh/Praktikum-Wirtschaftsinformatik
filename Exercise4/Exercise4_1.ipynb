{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "46b8a70b",
   "metadata": {},
   "outputs": [],
   "source": [
    "import logging\n",
    "import json\n",
    "import random\n",
    "import pydantic\n",
    "from pydantic import BaseModel, ValidationError\n",
    "from typing import Optional, List, Dict, Any\n",
    "from colorama import Fore\n",
    "from haystack import component, Pipeline\n",
    "from haystack.dataclasses import ChatMessage\n",
    "from haystack.components.builders import ChatPromptBuilder\n",
    "# Note: haystack_integrations must be installed to use OllamaChatGenerator\n",
    "from haystack_integrations.components.generators.ollama import OllamaChatGenerator"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "a79e6e91",
   "metadata": {},
   "outputs": [],
   "source": [
    "#1b select prefered data structure and define List named \"tasks\"\n",
    "\n",
    "from typing import List\n",
    "from pydantic import BaseModel\n",
    "\n",
    "class Tasks(BaseModel):\n",
    "    name: str\n",
    "\n",
    "\n",
    "class TasksData(BaseModel):\n",
    "    tasks: List[str]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "765a5d04",
   "metadata": {},
   "outputs": [],
   "source": [
    "json_schema = TasksData.model_json_schema()\n",
    "SCHEMA_STRING = json.dumps(json_schema)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "56dad658",
   "metadata": {},
   "outputs": [],
   "source": [
    "#1c\n",
    "\n",
    "@component\n",
    "class OutputValidator:\n",
    "    def __init__(self, pydantic_model: pydantic.BaseModel):\n",
    "        self.pydantic_model = pydantic_model\n",
    "        self.iteration_counter = 0\n",
    "\n",
    "    # Define the component output\n",
    "    @component.output_types(valid_replies=List[str], invalid_replies=Optional[List[str]], error_message=Optional[str])\n",
    "    def run(self, replies: List[ChatMessage]):\n",
    "\n",
    "        self.iteration_counter += 1\n",
    "\n",
    "        ## Try to parse the LLM's reply ##\n",
    "        # If the LLM's reply is a valid object, return `\"valid_replies\"`\n",
    "        try:\n",
    "            output_dict = json.loads(replies[0].text)\n",
    "            self.pydantic_model.parse_obj(output_dict)\n",
    "            print(\n",
    "                Fore.GREEN\n",
    "                + f\"OutputValidator at Iteration {self.iteration_counter}: Valid JSON from LLM - No need for looping: {replies[0]}\"\n",
    "            )\n",
    "            return {\"valid_replies\": replies}\n",
    "\n",
    "        # If the LLM's reply is corrupted or not valid, return \"invalid_replies\" and the \"error_message\" for LLM to try again\n",
    "        except (ValueError, ValidationError) as e:\n",
    "            print(\n",
    "                Fore.RED\n",
    "                + f\"OutputValidator at Iteration {self.iteration_counter}: Invalid JSON from LLM - Let's try again.\\n\"\n",
    "                f\"Output from LLM:\\n {replies[0]} \\n\"\n",
    "                f\"Error from OutputValidator: {e}\"\n",
    "            )\n",
    "            return {\"invalid_replies\": replies, \"error_message\": str(e)}\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "89277d5b",
   "metadata": {},
   "outputs": [],
   "source": [
    "output_validator = OutputValidator(pydantic_model=TasksData)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "9ca6d182",
   "metadata": {},
   "outputs": [],
   "source": [
    "#1d\n",
    "\n",
    "from haystack.components.builders import ChatPromptBuilder\n",
    "prompt = [\n",
    "    ChatMessage.from_user(\n",
    "        \"\"\"\n",
    "Extract all **task names** from the BPMN description provided in the passage: {{passage}}.\n",
    "\n",
    "Return the extracted task names as a single JSON object that adheres strictly to the following schema. Only return the actual task instances without including the schema definition itself.\n",
    "\n",
    "The required JSON schema is:\n",
    "{{json_schema}}\n",
    "\n",
    "For example, if the schema is `{\"tasks\": [\"task1\", \"task2\", ...]}` and the BPMN description contains \"Approve Claim\" and \"Notify Customer\", your output must be: `{\"tasks\": [\"Approve Claim\", \"Notify Customer\"]}`.\n",
    "\n",
    "Make sure your response is a valid JSON dict and not a list.\n",
    "{% if invalid_replies and error_message %}\n",
    "  You already created the following output in a previous attempt: {{invalid_replies}}\n",
    "  However, this doesn't comply with the format requirements from above and triggered this Python exception: {{error_message}}\n",
    "  Correct the output and try again. Just return the corrected output without any extra explanations.\n",
    "{% endif %}\n",
    "\"\"\"\n",
    "    )\n",
    "]\n",
    "\n",
    "prompt_builder = ChatPromptBuilder(template=prompt, required_variables=[\"json_schema\", \"passage\"])\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "d5718965",
   "metadata": {},
   "outputs": [],
   "source": [
    "#1e\n",
    "\n",
    "from haystack_integrations.components.generators.ollama import OllamaChatGenerator\n",
    "chat_generator = OllamaChatGenerator(model=\"llama3.2:3b\",\n",
    "url = \"http://localhost:11434\",\n",
    "timeout = 30*60,\n",
    "generation_kwargs={\n",
    "\"num_ctx\": 4096,\n",
    "\"temperature\": 0.9,\n",
    "})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "ddcd8704",
   "metadata": {},
   "outputs": [],
   "source": [
    "@component\n",
    "class MockChatGenerator:\n",
    "    @component.output_types(replies=List[ChatMessage])\n",
    "    def run(self, messages: List[ChatMessage]):\n",
    "        # Very naive: just returns a fixed valid JSON for demo purposes\n",
    "        reply_text = json.dumps({\n",
    "            \"tasks\": [\n",
    "                {\"name\": \"Task A\"},\n",
    "                {\"name\": \"Task B\"}\n",
    "            ]\n",
    "        })\n",
    "        return {\"replies\": [ChatMessage.from_assistant(reply_text)]}\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "80e7def0",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<haystack.core.pipeline.pipeline.Pipeline object at 0x7f607818a650>\n",
       "üöÖ Components\n",
       "  - prompt_builder: ChatPromptBuilder\n",
       "  - llm: OllamaChatGenerator\n",
       "  - output_validator: OutputValidator\n",
       "üõ§Ô∏è Connections\n",
       "  - prompt_builder.prompt -> llm.messages (list[ChatMessage])\n",
       "  - llm.replies -> output_validator.replies (List[ChatMessage])"
      ]
     },
     "execution_count": 45,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from haystack import Pipeline\n",
    "\n",
    "# 1. Define your three inputs (BPMN descriptions)\n",
    "# Assuming you are defining these variables in your Python script or notebook\n",
    "\n",
    "bpmn_text1_data = {\n",
    "    \"passage\": \"\"\"BPMN Model 1: \n",
    "        The process initiates when a request is submitted, leading immediately to the Initial Screening of the request details. The system then automatically Send Acknowledgment to the customer confirming receipt. Following this, the agent proceeds to Identify Task Type (e.g., bug, feature request, or maintenance). If the description is incomplete, the agent will Request Clarification from the submitter. Once the necessary information is gathered, a qualified manager will Assign Engineer based on project needs and skill set. The assigned engineer's first responsibility is to Create Solution Plan, which outlines the necessary steps and resources. Once the plan is approved, the engineer moves on to Implement Solution. Throughout the development phase, automated systems continually Monitor Progress and resource usage. Before the solution is deployed, a separate team must perform a rigorous Quality Assurance Check. The final required step is securing Final Customer Sign-off from the client, and only then does the process officially end.\"\"\"\n",
    "}\n",
    "\n",
    "bpmn_text2_data = {\n",
    "    \"passage\": \"\"\"BPMN Model 2:\n",
    "       The process initiates when a request is submitted, leading immediately to the \"Initial Screening\" of the request details. The system then automatically \"Send Acknowledgment\" to the customer confirming receipt. Following this, the agent proceeds to \"Identify Task Type\" (e.g., bug, feature request, or maintenance). If the description is incomplete, the agent will \"Request Clarification\" from the submitter. Once the necessary information is gathered, a qualified manager will \"Assign Engineer\" based on project needs and skill set. The assigned engineer's first responsibility is to \"Create Solution Plan\", which outlines the necessary steps and resources. Once the plan is approved, the engineer moves on to \"Implement Solution\". Throughout the development phase, automated systems continually \"Monitor Progress\" and resource usage. Before the solution is deployed, a separate team must perform a rigorous \"Quality Assurance Check\". The final required step is securing \"Final Customer Sign-off\" from the client, and only then does the process officially end\"\"\"\n",
    "}\n",
    "\n",
    "bpmn_text3_data = {\n",
    "    \"passage\": \"\"\"BPMN Model 3:\n",
    "        The process begins with \"Receive Order\", then \"Check Inventory\",\n",
    "        then \"Fulfill Order\", and finally \"Send Invoice\" before it ends.\"\"\"\n",
    "}\n",
    "\n",
    "# ‚≠êÔ∏è Combine them into the single iterable list expected by the loop:\n",
    "bpmn_texts = [\n",
    "    bpmn_text1_data,\n",
    "    bpmn_text2_data,\n",
    "    bpmn_text3_data\n",
    "]\n",
    "\n",
    "pipeline = Pipeline() \n",
    "\n",
    "pipeline.add_component(instance=prompt_builder, name=\"prompt_builder\")\n",
    "pipeline.add_component(instance=chat_generator , name=\"llm\")\n",
    "pipeline.add_component(instance=output_validator, name=\"output_validator\")\n",
    "\n",
    "pipeline.connect(\"prompt_builder.prompt\", \"llm.messages\")\n",
    "pipeline.connect(\"llm.replies\", \"output_validator\")\n",
    "# The other connections are REMOVED because the 'while' loop handles the retry data manually.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b2306fa0",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "acb97fb0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Starting Haystack Pipeline Execution...\n",
      "\n",
      "üöÄ Running Pipeline for BPMN Model 1 (Item 1/3)...\n",
      "‚ùå Run failed with unexpected error: The following component failed to run:\n",
      "Component name: 'llm'\n",
      "Component type: 'OllamaChatGenerator'\n",
      "Error: model \"llama3.2:3b\" not found, try pulling it first (status code: 404). Retrying... (Attempt 2/3)\n",
      "‚ùå Run failed with unexpected error: The following component failed to run:\n",
      "Component name: 'llm'\n",
      "Component type: 'OllamaChatGenerator'\n",
      "Error: model \"llama3.2:3b\" not found, try pulling it first (status code: 404). Retrying... (Attempt 3/3)\n",
      "‚ùå Run failed with unexpected error: The following component failed to run:\n",
      "Component name: 'llm'\n",
      "Component type: 'OllamaChatGenerator'\n",
      "Error: model \"llama3.2:3b\" not found, try pulling it first (status code: 404). Retrying... (Attempt 4/3)\n",
      "\n",
      "üöÄ Running Pipeline for BPMN Model 2 (Item 2/3)...\n",
      "‚ùå Run failed with unexpected error: The following component failed to run:\n",
      "Component name: 'llm'\n",
      "Component type: 'OllamaChatGenerator'\n",
      "Error: model \"llama3.2:3b\" not found, try pulling it first (status code: 404). Retrying... (Attempt 2/3)\n",
      "‚ùå Run failed with unexpected error: The following component failed to run:\n",
      "Component name: 'llm'\n",
      "Component type: 'OllamaChatGenerator'\n",
      "Error: model \"llama3.2:3b\" not found, try pulling it first (status code: 404). Retrying... (Attempt 3/3)\n",
      "‚ùå Run failed with unexpected error: The following component failed to run:\n",
      "Component name: 'llm'\n",
      "Component type: 'OllamaChatGenerator'\n",
      "Error: model \"llama3.2:3b\" not found, try pulling it first (status code: 404). Retrying... (Attempt 4/3)\n",
      "\n",
      "üöÄ Running Pipeline for BPMN Model 3 (Item 3/3)...\n",
      "‚ùå Run failed with unexpected error: The following component failed to run:\n",
      "Component name: 'llm'\n",
      "Component type: 'OllamaChatGenerator'\n",
      "Error: model \"llama3.2:3b\" not found, try pulling it first (status code: 404). Retrying... (Attempt 2/3)\n",
      "‚ùå Run failed with unexpected error: The following component failed to run:\n",
      "Component name: 'llm'\n",
      "Component type: 'OllamaChatGenerator'\n",
      "Error: model \"llama3.2:3b\" not found, try pulling it first (status code: 404). Retrying... (Attempt 3/3)\n",
      "‚ùå Run failed with unexpected error: The following component failed to run:\n",
      "Component name: 'llm'\n",
      "Component type: 'OllamaChatGenerator'\n",
      "Error: model \"llama3.2:3b\" not found, try pulling it first (status code: 404). Retrying... (Attempt 4/3)\n",
      "\n",
      "--- Manual Verification Setup ---\n",
      "All Run Results: ['Failed to produce valid output after 3 attempts for BPMN Model 1.', 'Failed to produce valid output after 3 attempts for BPMN Model 2.', 'Failed to produce valid output after 3 attempts for BPMN Model 3.']\n",
      "\n",
      "You must now manually check each result against the ground truth for each BPMN model.\n",
      "This involves calculating **Precision** and **Recall** for the extracted task names.\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "# Assuming bpmn_texts, pipeline, and SCHEMA_STRING are defined and accessible\n",
    "\n",
    "MAX_RERUNS = 3 \n",
    "all_results = []\n",
    "ground_truth = [] \n",
    "\n",
    "print(\"Starting Haystack Pipeline Execution...\")\n",
    "\n",
    "# --- CORRECTED: Execution Loop ---\n",
    "# The loop iterates over each dictionary in bpmn_texts. \n",
    "# We call the iteration variable 'bpmn_text_data' for clarity.\n",
    "for i, bpmn_text_data in enumerate(bpmn_texts):\n",
    "    \n",
    "    # --- 1. Model Name and Logging (Fixed Logic) ---\n",
    "    # Extract model name from the passage for logging.\n",
    "    passage_text = bpmn_text_data[\"passage\"]\n",
    "    \n",
    "    # Try to extract \"BPMN Model X\" or default to \"Model Y\"\n",
    "    model_name_match = passage_text.split(\":\", 1)[0].strip()\n",
    "    model_name = model_name_match if model_name_match.startswith(\"BPMN Model\") else f\"Model {i+1}\"\n",
    "    \n",
    "    print(f\"\\nüöÄ Running Pipeline for {model_name} (Item {i+1}/{len(bpmn_texts)})...\")\n",
    "    \n",
    "    # --- 2. Initialize Variables for THIS Run ---\n",
    "    # Initialization MUST occur inside the loop for each new model.\n",
    "    input_data = {\n",
    "        \"passage\": passage_text,\n",
    "        \"json_schema\": SCHEMA_STRING,\n",
    "        \"invalid_replies\": None,\n",
    "        \"error_message\": None\n",
    "    }\n",
    "    \n",
    "    attempt = 0\n",
    "    successful_run = False\n",
    "    run_output = None\n",
    "    \n",
    "    # --- 3. Start Retry Loop (Applied to THIS Model) ---\n",
    "    while not successful_run and attempt < MAX_RERUNS:\n",
    "        try:\n",
    "            # Run the pipeline with the current state of input_data\n",
    "            run_output = pipeline.run(data={\"prompt_builder\": input_data})\n",
    "            \n",
    "            validator_output = run_output[\"output_validator\"]\n",
    "            \n",
    "            # Check for successful run via the 'result' key\n",
    "            if validator_output.get(\"result\"):\n",
    "                 successful_run = True\n",
    "                 print(f\"‚úÖ Success on attempt {attempt + 1}.\")\n",
    "                 all_results.append(validator_output.get(\"result\"))\n",
    "            else:\n",
    "                 # --- Manual Update for Retry Inputs ---\n",
    "                 # Pass the error information back to prompt_builder for the next attempt\n",
    "                 input_data[\"invalid_replies\"] = validator_output.get(\"invalid_replies\")\n",
    "                 input_data[\"error_message\"] = validator_output.get(\"error_message\")\n",
    "                 # --- END FIX ---\n",
    "                 \n",
    "                 attempt += 1\n",
    "                 print(f\"‚ö†Ô∏è Run failed (validation error). Retrying... (Attempt {attempt + 1}/{MAX_RERUNS})\")\n",
    "                 \n",
    "        except Exception as e:\n",
    "            # Catch unexpected exceptions (e.g., LLM connection or server errors)\n",
    "            attempt += 1\n",
    "            print(f\"‚ùå Run failed with unexpected error: {e}. Retrying... (Attempt {attempt + 1}/{MAX_RERUNS})\")\n",
    "            # For unexpected errors, reset the retry inputs\n",
    "            input_data[\"invalid_replies\"] = None\n",
    "            input_data[\"error_message\"] = f\"An unexpected error occurred: {e}\"\n",
    "            \n",
    "    # --- 4. Final Result Recording for THIS Model ---\n",
    "    if not successful_run:\n",
    "        all_results.append(f\"Failed to produce valid output after {MAX_RERUNS} attempts for {model_name}.\")\n",
    "\n",
    "# --- Verification Setup (Outside the loop) ---\n",
    "print(\"\\n--- Manual Verification Setup ---\")\n",
    "print(\"All Run Results:\", all_results)\n",
    "print(\"\\nYou must now manually check each result against the ground truth for each BPMN model.\")\n",
    "print(\"This involves calculating **Precision** and **Recall** for the extracted task names.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "d864599e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "--- Precision and Recall Scoring Results ---\n",
      "Note: Scoring assumes extracted task names are case-insensitive matches.\n",
      "\n",
      "BPMN Model 1\n",
      "  > Precision: 1.0\n",
      "  > Recall:    1.0\n",
      "  > Metrics:   TP=10, FP=0, FN=0\n",
      "  > Counts:    Extracted=10, Ground Truth=10\n",
      "\n",
      "BPMN Model 2\n",
      "  > Precision: 0.8889\n",
      "  > Recall:    0.8\n",
      "  > Metrics:   TP=8, FP=1, FN=2\n",
      "  > Counts:    Extracted=9, Ground Truth=10\n",
      "\n",
      "BPMN Model 3\n",
      "  > Precision: 1.0\n",
      "  > Recall:    0.75\n",
      "  > Metrics:   TP=3, FP=0, FN=1\n",
      "  > Counts:    Extracted=3, Ground Truth=4\n"
     ]
    }
   ],
   "source": [
    "import json\n",
    "from typing import List, Dict, Any, Optional\n",
    "\n",
    "# --- 1. Define Ground Truth Data ---\n",
    "\n",
    "# Ground truth for BPMN Model 1 & 2 (10 tasks)\n",
    "gt_model_1_2 = [\n",
    "    \"Initial Screening\",\n",
    "    \"Send Acknowledgment\",\n",
    "    \"Identify Task Type\",\n",
    "    \"Request Clarification\",\n",
    "    \"Assign Engineer\",\n",
    "    \"Create Solution Plan\",\n",
    "    \"Implement Solution\",\n",
    "    \"Monitor Progress\",\n",
    "    \"Quality Assurance Check\",\n",
    "    \"Final Customer Sign-off\"\n",
    "]\n",
    "\n",
    "# Ground truth for BPMN Model 3 (4 tasks)\n",
    "gt_model_3 = [\n",
    "    \"Receive Order\",\n",
    "    \"Check Inventory\",\n",
    "    \"Fulfill Order\",\n",
    "    \"Send Invoice\"\n",
    "]\n",
    "\n",
    "# Combined ground truth list, matching the order of execution\n",
    "GROUND_TRUTH = [\n",
    "    gt_model_1_2,  # Corresponds to Model 1 output\n",
    "    gt_model_1_2,  # Corresponds to Model 2 output\n",
    "    gt_model_3     # Corresponds to Model 3 output\n",
    "]\n",
    "\n",
    "# --- 2. Define Placeholder Results (Demonstration Data) ---\n",
    "\n",
    "# NOTE: Replace this PLACEHOLDER_ALL_RESULTS list with your actual 'all_results' \n",
    "# list from your successful pipeline execution.\n",
    "PLACEHOLDER_ALL_RESULTS = [\n",
    "    # Model 1: Perfect match (10 tasks)\n",
    "    {'tasks': [{'name': t} for t in GROUND_TRUTH[0]]},\n",
    "\n",
    "    # Model 2: Misses \"Request Clarification\" and \"Final Customer Sign-off\", adds \"Wrong Task\"\n",
    "    # (FP=1, FN=2, TP=8)\n",
    "    {'tasks': [\n",
    "        {'name': 'Initial Screening'}, \n",
    "        {'name': 'Identify Task Type'}, \n",
    "        {'name': 'Assign Engineer'}, \n",
    "        {'name': 'Monitor Progress'}, \n",
    "        {'name': 'Send Acknowledgment'}, \n",
    "        {'name': 'Create Solution Plan'}, \n",
    "        {'name': 'Implement Solution'}, \n",
    "        {'name': 'Quality Assurance Check'}, \n",
    "        {'name': 'Wrong Task Added'}\n",
    "    ]},\n",
    "\n",
    "    # Model 3: Misses 1 task (\"Send Invoice\") (FP=0, FN=1, TP=3)\n",
    "    {'tasks': [\n",
    "        {'name': 'Receive Order'}, \n",
    "        {'name': 'Check Inventory'}, \n",
    "        {'name': 'Fulfill Order'}\n",
    "    ]}\n",
    "]\n",
    "\n",
    "\n",
    "# --- 3. Calculation Function ---\n",
    "\n",
    "def calculate_precision_recall(\n",
    "    predicted_output: Dict[str, Any], \n",
    "    ground_truth_tasks: List[str], \n",
    "    case_sensitive: bool = False\n",
    ") -> Dict[str, Optional[float]]:\n",
    "    \"\"\"\n",
    "    Calculates precision and recall based on extracted task names.\n",
    "    \"\"\"\n",
    "    # 1. Extract predicted tasks from the LLM output structure\n",
    "    if 'tasks' not in predicted_output or not isinstance(predicted_output['tasks'], list):\n",
    "        print(f\"Warning: Invalid output structure: {predicted_output}. Cannot score.\")\n",
    "        return {\"precision\": None, \"recall\": None, \"TP\": 0, \"FP\": 0, \"FN\": 0}\n",
    "        \n",
    "    predicted_tasks = [t.get('name', '').strip() for t in predicted_output['tasks'] if t.get('name')]\n",
    "\n",
    "    # 2. Normalize case for comparison\n",
    "    if not case_sensitive:\n",
    "        predicted_set = {t.lower() for t in predicted_tasks}\n",
    "        ground_set = {t.lower() for t in ground_truth_tasks}\n",
    "    else:\n",
    "        predicted_set = set(predicted_tasks)\n",
    "        ground_set = set(ground_truth_tasks)\n",
    "\n",
    "    # 3. Calculate metrics\n",
    "    TP = len(predicted_set.intersection(ground_set))\n",
    "    FP = len(predicted_set.difference(ground_set))\n",
    "    FN = len(ground_set.difference(predicted_set))\n",
    "\n",
    "    # 4. Calculate Precision and Recall\n",
    "    precision = TP / (TP + FP) if (TP + FP) > 0 else 0.0\n",
    "    recall = TP / (TP + FN) if (TP + FN) > 0 else 0.0\n",
    "\n",
    "    return {\n",
    "        \"precision\": round(precision, 4), \n",
    "        \"recall\": round(recall, 4), \n",
    "        \"TP\": TP, \n",
    "        \"FP\": FP, \n",
    "        \"FN\": FN\n",
    "    }\n",
    "\n",
    "\n",
    "# --- 4. Run Calculation and Display Results ---\n",
    "\n",
    "# Choose the data source: use PLACEHOLDER_ALL_RESULTS for demonstration\n",
    "# If your pipeline runs successfully, use: all_results_to_score = all_results \n",
    "all_results_to_score = PLACEHOLDER_ALL_RESULTS\n",
    "\n",
    "print(\"\\n--- Precision and Recall Scoring Results ---\")\n",
    "print(\"Note: Scoring assumes extracted task names are case-insensitive matches.\")\n",
    "\n",
    "scoring_results = []\n",
    "\n",
    "for i, (result, truth) in enumerate(zip(all_results_to_score, GROUND_TRUTH)):\n",
    "    \n",
    "    # Skip models that failed to produce structured output\n",
    "    if isinstance(result, str):\n",
    "        scoring_results.append({\n",
    "            \"Model\": f\"Model {i+1}\",\n",
    "            \"Status\": result\n",
    "        })\n",
    "        continue\n",
    "\n",
    "    scores = calculate_precision_recall(result, truth, case_sensitive=False)\n",
    "    \n",
    "    scoring_results.append({\n",
    "        \"Model\": f\"BPMN Model {i+1}\",\n",
    "        \"Precision\": scores['precision'],\n",
    "        \"Recall\": scores['recall'],\n",
    "        \"TP\": scores['TP'],\n",
    "        \"FP\": scores['FP'],\n",
    "        \"FN\": scores['FN'],\n",
    "        \"Extracted Tasks Count\": len(result.get('tasks', [])),\n",
    "        \"Ground Truth Count\": len(truth)\n",
    "    })\n",
    "\n",
    "# Display results in a structured format\n",
    "for score_data in scoring_results:\n",
    "    if 'Status' in score_data:\n",
    "        print(f\"\\n{score_data['Model']}: {score_data['Status']}\")\n",
    "    else:\n",
    "        print(f\"\\n{score_data['Model']}\")\n",
    "        print(f\"  > Precision: {score_data['Precision']}\")\n",
    "        print(f\"  > Recall:    {score_data['Recall']}\")\n",
    "        print(f\"  > Metrics:   TP={score_data['TP']}, FP={score_data['FP']}, FN={score_data['FN']}\")\n",
    "        print(f\"  > Counts:    Extracted={score_data['Extracted Tasks Count']}, Ground Truth={score_data['Ground Truth Count']}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "0becba8d",
   "metadata": {},
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "'NoneType' object is not subscriptable",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mTypeError\u001b[39m                                 Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[48]\u001b[39m\u001b[32m, line 1\u001b[39m\n\u001b[32m----> \u001b[39m\u001b[32m1\u001b[39m valid_reply = \u001b[43mrun_output\u001b[49m\u001b[43m[\u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43moutput_validator\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m]\u001b[49m[\u001b[33m\"\u001b[39m\u001b[33mresult\u001b[39m\u001b[33m\"\u001b[39m][\u001b[33m\"\u001b[39m\u001b[33mtasks\u001b[39m\u001b[33m\"\u001b[39m]\n\u001b[32m      2\u001b[39m \u001b[38;5;66;03m#valid_json = json.loads(valid_reply)\u001b[39;00m\n\u001b[32m      3\u001b[39m \u001b[38;5;28mprint\u001b[39m(valid_reply)\n",
      "\u001b[31mTypeError\u001b[39m: 'NoneType' object is not subscriptable"
     ]
    }
   ],
   "source": [
    "valid_reply = run_output[\"output_validator\"][\"result\"][\"tasks\"]\n",
    "#valid_json = json.loads(valid_reply)\n",
    "print(valid_reply)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
